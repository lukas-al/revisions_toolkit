{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisions Toolkit\n",
    "This project is intended to help with both obtaining and constructing revisions series for a number of GDP components from the ONS. It's not doing anything revolutionary.\n",
    "\n",
    "In the 'old' folder it is structured as a Kedro project. Kedro has some little issues it seems with the NAS drive and symlinks, so have had to port it into one big jupyter notebook. Have opened an issue on the Kedro project with a proposed [fix](https://github.com/kedro-org/kedro/issues/3694)\n",
    "\n",
    "\n",
    "## Outline\n",
    "For the project:\n",
    "1. Look to the ONS website and scrape the latest spreadsheets\n",
    "2. Clean them up and extract the data\n",
    "3. Construct the revisions series\n",
    "4. Save the data\n",
    "\n",
    "## How to run the project\n",
    "Make sure you've created an environment with the dependencies in `requirements.txt`. If not, you can do this by running the following command:\n",
    "\n",
    "```miniconda3\n",
    "conda create --name <env_name> \n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "To make this easier to do, I'd recommend using Visual Studio Code to open the terminal in the correct folder. Otherwise you can copy the path.\n",
    "\n",
    "The project is now a simple jupyter notebook, so you should be able to run it as usual, by selecting the correct kernel in the top right.\n",
    "\n",
    "## Common errors\n",
    "If you're having trouble running the project, you might want to check the following:\n",
    "\n",
    "1. Make sure you're in the right environment. You can check this by running `conda env list` and seeing which environment is active.\n",
    "\n",
    "2. The ONS may have changed the format of their spreadsheets, which could cause the scraping to fail. There isn't an easy way to check for this, but you can look at the logs to see if and when the scraping is failing.\n",
    "\n",
    "3. Sometimes the initial download might fail or be blocked by the ONS website. It might also be an issue with the Bank's firewall, though those issues should be resolved?\n",
    "\n",
    "## Layout\n",
    "The rest of this project is structured as follows:\n",
    "\n",
    "1. Logic\n",
    "Area for functions and classes which we use later on to be defined.\n",
    "\n",
    "2. Config\n",
    "Configuration area for the project\n",
    "\n",
    "3. Pipelines\n",
    "Some simple pipelines to structure how we run the ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Logic\n",
    "Contains the functions and classes we need to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePosixPath\n",
    "from typing import Any, Dict\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import boerequests\n",
    "    sys.modules[\"requests\"] = boerequests # Replace all requests with boerequests\n",
    "except:\n",
    "    import requests as boerequests\n",
    "\n",
    "\n",
    "class GDPVintage():\n",
    "    \"\"\"\n",
    "    A class representing the GDP vintages (revision triangles) dataset.\n",
    "\n",
    "    Attributes:\n",
    "        _writepath (PurePosixPath): The write path for saving the dataset.\n",
    "        _base_url (str): The base URL for downloading the dataset.\n",
    "        _name (str): The name of the dataset.\n",
    "        _extracted_files (List[str]): The list of extracted files from the dataset.\n",
    "        _release_date (str): The release date of the dataset.\n",
    "\n",
    "    Methods:\n",
    "        load() -> Dict[str, pd.DataFrame]:\n",
    "            Loads the GDP vintages (revision triangles) data from the ONS website.\n",
    "\n",
    "        save(data: Dict[str, pd.DataFrame]) -> None:\n",
    "            Saves the data to the specified filepath.\n",
    "\n",
    "        describe() -> Dict[str, Any]:\n",
    "            Returns a dictionary that describes the attributes of the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, writepath: str, base_url: str, dataset_name: str):\n",
    "        self._writepath = PurePosixPath(writepath)\n",
    "        self._base_url = base_url\n",
    "        self._name = dataset_name\n",
    "        self._extracted_files = None\n",
    "        self._release_date = None\n",
    "        \n",
    "    def load(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Loads the GDP vintages (revision triangles) data from the ONS website.\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary containing the dataframes for each sheet in the downloaded release.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If the expected file is missing in the downloaded release.\n",
    "            ConnectionError: If there is an issue with downloading the release or accessing the data page.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Downloading the latest release from the ONS website...\")\n",
    "        \n",
    "        # Introduce a short random delay to prevent concurrent Airflow tasks from hitting the same URL at the same time\n",
    "        delay = random.uniform(0, 5)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Send a GET request to the ONS website\n",
    "        response = boerequests.get(self._base_url, timeout=15)\n",
    "        \n",
    "        # Check if the response was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Find the latest release URL from the page content. \n",
    "            # This logic assumes the release URL is the first one found on the page.\n",
    "            pattern = r'<a href=\"(/file\\?uri=.*?)\".*?>'\n",
    "            latest_release_url = re.search(pattern, response.text).group(1)\n",
    "            latest_release_url = \"https://www.ons.gov.uk\" + latest_release_url\n",
    "            print(\"Found the latest release URL: \" + latest_release_url)\n",
    "            \n",
    "            # Try get the release date from the URL\n",
    "            try:\n",
    "                quarter = re.findall(r\"quarter(\\d)\", latest_release_url)\n",
    "                year = re.findall(r\"(\\d{4})\", latest_release_url)\n",
    "                self._release_date = \"Q\" + quarter[0] + \" \" + year[0]\n",
    "            except:\n",
    "                self._release_date = None\n",
    "            \n",
    "            # Send a GET request to the latest release URL and get the response\n",
    "            response = boerequests.get(latest_release_url, timeout=15)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "\n",
    "                # Load the data as a dictionary to hold each sheet within a df\n",
    "                data = {}\n",
    "                # If the content header is a zip file, load the data using the zipfile module\n",
    "                if \"application/zip\" in response.headers.get(\"content-type\"):\n",
    "                    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                        for filename in z.namelist():\n",
    "                            with z.open(filename) as f:\n",
    "                                data[filename] = pd.read_excel(f, sheet_name=None)\n",
    "                elif \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\" in response.headers.get(\"content-type\"):\n",
    "                    data[latest_release_url.split(\"/\")[-1]] = pd.read_excel(io.BytesIO(response.content), sheet_name=None)\n",
    "                \n",
    "                ## SAVE THE DATA\n",
    "                print(\"Saving the raw data...\")\n",
    "                self.save(data)\n",
    "                \n",
    "                # Return the data as a dictionary of dataframes\n",
    "                return data\n",
    "\n",
    "            else:\n",
    "                print(\"[bold red blink]Failed to download the latest release due to status code: \" + str(response.status_code), extra={\"markup\": True})\n",
    "                raise ConnectionError\n",
    "        else:\n",
    "            print(\"[bold red blink]Failed to access the data page due to status code: \" + str(response.status_code), extra={\"markup\": True})\n",
    "            raise ConnectionError\n",
    "        \n",
    "    def save(self, data: Dict[str, pd.DataFrame]) -> None:\n",
    "            \"\"\"\n",
    "            Saves the data to the specified filepath.\n",
    "\n",
    "            Args:\n",
    "                data (Dict[str, pd.DataFrame]): A dictionary containing the data to be saved.\n",
    "                    The keys represent the filenames, and the values represent the corresponding\n",
    "                    pandas DataFrame objects.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "            \"\"\"\n",
    "\n",
    "            for filename, df_dict in data.items():      \n",
    "                output_file_path = self._writepath / filename\n",
    "                \n",
    "                # Create the directory if it doesn't exist\n",
    "                os.makedirs(output_file_path.parent, exist_ok=True)\n",
    "                \n",
    "                with pd.ExcelWriter(output_file_path) as writer:\n",
    "                    for sheet_name, sheet_df in df_dict.items():\n",
    "                        sheet_df.to_excel(writer, sheet_name=sheet_name)\n",
    "                                    \n",
    "                                    \n",
    "            self._extracted_files = list(data.keys())  # Log the list of extracted files\n",
    "            print(f\"Successfully saved the latest {self._name} data release from the ONS website.\")\n",
    "\n",
    "    def describe(self) -> Dict[str, Any]:\n",
    "            \"\"\"Returns a dictionary that describes the attributes of the dataset.\n",
    "\n",
    "            Returns:\n",
    "                A dictionary containing the following attributes:\n",
    "                - folderpath: The folder path of the dataset.\n",
    "                - release_date: The release date of the dataset.\n",
    "                - extracted_files: The list of extracted files from the dataset.\n",
    "                - name: The name of the dataset.\n",
    "            \"\"\"\n",
    "            \n",
    "            output_dict = dict(\n",
    "                folderpath=self._writepath, \n",
    "                release_date=self._release_date, \n",
    "                extracted_files=self._extracted_files,\n",
    "                name=self._name\n",
    "            )\n",
    "            \n",
    "            return output_dict\n",
    "        \n",
    "# # # # Convenience functions\n",
    "def get_latest_data(base_url):\n",
    "    \"\"\"\n",
    "    Retrieves the latest data from a given base URL by iterating back in months and testing URLs.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL to construct the test URLs.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The response object containing the data if a working URL is found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no working URL is found.\n",
    "    \"\"\"\n",
    "    # Get the current date\n",
    "    current_date = pd.Timestamp.now()\n",
    "\n",
    "    # Adjust the current date to the midpoint of the current month. Might avoid some datetime weirdness\n",
    "    current_date = current_date.replace(day=15)\n",
    "\n",
    "    # Iterate back in months from the current date\n",
    "    for i in range(0, 12):\n",
    "        time.sleep(0.3)\n",
    "        # Calculate the date to test\n",
    "        test_date = current_date - pd.DateOffset(months=i)\n",
    "\n",
    "        # Construct the URL to test\n",
    "        test_url = f\"{base_url}{test_date.strftime('%b%y').lower()}.zip\"\n",
    "\n",
    "        # Send a request to the URL\n",
    "        response = boerequests.get(test_url)\n",
    "\n",
    "        # Check if the response is successful\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "\n",
    "    raise ValueError(\"No working URL found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Nodes are the building blocks of pipelines, and represent tasks. \n",
    "Pipelines are used to combine nodes to build workflows, which range from simple data engineering workflows \n",
    "to end-to-end production workflows.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def load_data(\n",
    "    data_dict: dict, \n",
    "    list_of_filenames_to_load: List[str]\n",
    ") -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the quarterly data from the given data dictionary and return the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dict (dict): A dictionary containing the data to be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Loading the data...\")\n",
    "    df_holder = []\n",
    "    \n",
    "    for filename, dict_of_sheets in data_dict.items(): # For file extracted from the zip\n",
    "        for file_to_load in list_of_filenames_to_load: # For the list of files to load in config\n",
    "            if file_to_load.lower() in filename.lower(): # If the names match\n",
    "                for sheet_name, df in dict_of_sheets.items(): # For the sheets in the file\n",
    "                    if \"triangle\" in sheet_name.lower(): # If the sheet name contains \"triangle\"\n",
    "                        df_holder.append(df) # Append to a list of returns\n",
    "                    elif sheet_name.lower() == \"estimate\": # If the sheet name is \"estimate\"\n",
    "                        df_holder.append(df) # Append to a list of returns\n",
    "    \n",
    "    return df_holder\n",
    "\n",
    "\n",
    "def clean_quarterly_data(data_list: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the given GDP vintages dataset by performing the following steps:\n",
    "    1. Drops unnecessary rows.\n",
    "    2. Replaces the index with the first column.\n",
    "    3. Drops the first column.\n",
    "    4. Sets the first row as the column names.\n",
    "    5. Drops the first row.\n",
    "    6. Rotates the table.\n",
    "    7. Replaces empty data with NaNs.\n",
    "\n",
    "    Parameters:\n",
    "    - gdp_vintages_ons (pandas.DataFrame): The Q-GDP vintages dataset to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The cleaned Q-GDP vintages dataset.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning the data...\")\n",
    "    \n",
    "    clean_data_list = []\n",
    "    \n",
    "    for data in data_list:\n",
    "    \n",
    "        # Drop some unnecessary rows\n",
    "        data = data.drop(\n",
    "            data.index[[0,1,3,4,5,-1]]\n",
    "        )\n",
    "        # Replace the index with the first column\n",
    "        data.index = data.iloc[:, 0]\n",
    "        \n",
    "        # Drop the first column now it's been moved\n",
    "        data = data.drop(data.columns[0], axis=1)\n",
    "        \n",
    "        # Make the first row into the names of the columns\n",
    "        data.columns = data.iloc[0]\n",
    "        \n",
    "        # Drop the first row\n",
    "        data = data.drop(data.index[0])\n",
    "\n",
    "        # Rotate the table\n",
    "        data = data.T\n",
    "        \n",
    "        # Replace empty data with NaNs\n",
    "        pd.set_option('future.no_silent_downcasting', True) # Silly warning\n",
    "        data = data.replace(' ', pd.NA)\n",
    "        \n",
    "        # Turn the index into a datetime index which excel can read\n",
    "        data.index = data.index.map(lambda x: x.replace('Q1', '01').replace('Q2', '04').replace('Q3', '07').replace('Q4', '10'))\n",
    "       \n",
    "        with warnings.catch_warnings():  # Suppress the warning about not specifying a format\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            data.index = pd.to_datetime(data.index).to_period('Q') \n",
    "        \n",
    "        \n",
    "        clean_data_list.append(data)\n",
    "    \n",
    "    return clean_data_list\n",
    "\n",
    "\n",
    "def transform_and_combine(data_list: List[pd.DataFrame]) -> dict:\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame by constructing revision series for different quarters\n",
    "    and combines them into a new DataFrame along with the original data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the revisions triangle.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The combined DataFrame containing the revisions triangle and the revision series.\n",
    "    \"\"\"\n",
    "    t_data_list = []\n",
    "    print(\"Transforming the data...\")\n",
    "    \n",
    "    for data in data_list:\n",
    "            \n",
    "        transformed_revisions = pd.DataFrame(index=data.index)\n",
    "        \n",
    "        transformed_revisions[\"First Estimate\"] = construct_revision_series(data, 0).values\n",
    "        transformed_revisions[\"1st Period\"] = construct_revision_series(data, 1).values\n",
    "        transformed_revisions[\"2nd Period\"] = construct_revision_series(data, 2).values\n",
    "        transformed_revisions[\"3rd Period\"] = construct_revision_series(data, 3).values\n",
    "        transformed_revisions[\"4th Period\"] = construct_revision_series(data, 4).values\n",
    "        transformed_revisions[\"12th Period\"] = construct_revision_series(data, 12).values\n",
    "        transformed_revisions[\"36th Period\"] = construct_revision_series(data, 36).values\n",
    "        \n",
    "        total_gdp_vintage = {\n",
    "            'Revisions triangle': data,\n",
    "            'Revisions series': transformed_revisions\n",
    "        }\n",
    "        \n",
    "        t_data_list.append(total_gdp_vintage)\n",
    "        \n",
    "        \n",
    "    return t_data_list\n",
    "\n",
    "\n",
    "def save_data(data_list: List[dict], input_name_list: List[str], save_path: str) -> None:\n",
    "    \"\"\"Save the data to the specified filepath.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data to be saved.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Saving the data...\")\n",
    "    \n",
    "    save_path = PurePosixPath(save_path)\n",
    "    \n",
    "    for data, name in zip(data_list, input_name_list):\n",
    "\n",
    "        output_path = save_path / (name+'_PROCESSED.xlsx')\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(output_path.parent, exist_ok=True)\n",
    "\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            for sheet_name, df in data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=True)\n",
    "            \n",
    "    \n",
    "\n",
    "def construct_revision_series(revisions_triangle: pd.DataFrame, periods: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Construct the revision series from the revisions triangle, given the specified period.\n",
    "    Convenience function used in the transform_and_merge node.\n",
    "    \n",
    "    Parameters:\n",
    "        revisions_triangle (pd.DataFrame): The revisions triangle containing the estimates over time.\n",
    "        periods (int): The number of periods to consider for the revision series.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: The revision series calculated based on the specified period.\n",
    "        \n",
    "    If the passed period = 0, then the first estimate series is returned.\n",
    "    \"\"\"\n",
    "    revision_series = pd.Series(index=revisions_triangle.index, name=f\"{periods}_period_revision_series\")\n",
    "    \n",
    "    if periods == 0:\n",
    "        # Assume the user wants the first estimate series\n",
    "        for idx in revisions_triangle.index:\n",
    "            # Get the first estimate\n",
    "            try:\n",
    "                first_estimate = revisions_triangle.loc[idx].dropna().iloc[0]\n",
    "            except IndexError:\n",
    "                first_estimate = pd.NA   \n",
    "                \n",
    "            revision_series.loc[idx] = first_estimate         \n",
    "    else:\n",
    "        # Calculate the revisions series\n",
    "        for idx in revisions_triangle.index:\n",
    "            \n",
    "            # Get the first estimate            \n",
    "            # Get the estimate relevant to the period\n",
    "            try:\n",
    "                first_estimate = revisions_triangle.loc[idx].dropna().iloc[0]\n",
    "                final_estimate = revisions_triangle.loc[idx].dropna().iloc[periods]\n",
    "            except IndexError:\n",
    "                first_estimate = pd.NA\n",
    "                final_estimate = pd.NA\n",
    "\n",
    "            # Calculate the revision series\n",
    "            if final_estimate is pd.NA:\n",
    "                revision_series.loc[idx] = pd.NA\n",
    "            else:\n",
    "                revision_series.loc[idx] = round(final_estimate - first_estimate, 3)\n",
    "\n",
    "    return revision_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_monthly_data(df_list: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Clean the monthly data by performing the following steps:\n",
    "    1. Set the index of the DataFrame to the first column.\n",
    "    2. Drop the first two rows.\n",
    "    3. Drop the first column.\n",
    "    4. Set the column names to the values in the first row.\n",
    "    5. Drop the first row.\n",
    "    6. Drop the second row.\n",
    "    7. Transpose the DataFrame.\n",
    "    8. Drop the last column.\n",
    "    9. Clean the index by removing non-alphanumeric characters and converting it to datetime format.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the monthly data.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df_holder = []\n",
    "        \n",
    "    for df in df_list:\n",
    "        df.index = df.iloc[:, 0]\n",
    "        df = df.drop(index=df.index[0:2])\n",
    "        df = df.drop(columns=[df.columns[0]])\n",
    "        df.columns = df.iloc[0, :]\n",
    "        df = df.drop(index=df.index[0])\n",
    "        df = df.drop(index=df.index[0])\n",
    "        df = df.T\n",
    "        df = df.drop(columns=[df.columns[-1]])\n",
    "        \n",
    "        clean_index = df.index.map(lambda x: ' '.join(re.findall(r'[A-Za-z]+|\\d+', x)))\n",
    "        df.index = pd.to_datetime(clean_index, format='%Y %b')\n",
    "\n",
    "        df_holder.append(df)\n",
    "        \n",
    "    return df_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration\n",
    "Here is where we configure our project, using some of the functions we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup for our scraping of the datasets\n",
    "\n",
    "\n",
    "headline_qgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/income/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstrianglesforukgdpabmi\",\n",
    "    dataset_name=\"income Vintages\"\n",
    ")\n",
    "\n",
    "expenditure_qgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/expenditure/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstrianglesforcomponentsfortheexpenditureapproachtothemeasureofukgdp\",\n",
    "    dataset_name=\"Expenditure Vintages\"\n",
    ")\n",
    "\n",
    "deflator_qgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/deflator/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstrianglesforukgdpdeflatorquarteronquarterayearago\",\n",
    "    dataset_name=\"Deflator Vintages\"\n",
    ")\n",
    "\n",
    "income_mgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/headline/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstrianglesformonthlygdp\",\n",
    "    dataset_name=\"Monthly Vintages\"\n",
    ")\n",
    "\n",
    "income_qgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/investment/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstrianglesforcomponentsfortheincomeapproachtothemeasureofukgdp\",\n",
    "    dataset_name=\"Income Vintages\"\n",
    ")\n",
    "\n",
    "investment_qgdp_vintages = GDPVintage(\n",
    "    writepath=\"data/01_raw/investment/\",\n",
    "    base_url=\"https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/revisionstogrossfixedcapitalformationandbusinessinvestment\",\n",
    "    dataset_name=\"Investment Vintages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup for loading the correct spreadsheets from the downloaded files\n",
    "\n",
    "# THIS MATCHES THE FILENAMES IN THE DOWNLOADED ZIP FOLDERS WE WANT TO PROCESS\n",
    "headline_qgdp_filename_list = [\n",
    "    \"ABMI - Quarterly GDP at Market Prices\"\n",
    "]\n",
    "headline_qgdp_finalpath = \"data/02_intermediate/headline/\"\n",
    "\n",
    "# THIS MATCHES THE FILENAMES IN THE DOWNLOADED ZIP FOLDERS WE WANT TO PROCESS\n",
    "headline_mgdp_filename_list = [\n",
    "    \"mgdp revision triangle (m on m)\", \n",
    "    \"mgdp revision triangle (3m on 3m)\"\n",
    "]\n",
    "headline_mgdp_finalpath = \"data/02_intermediate/headline/\"\n",
    "\n",
    "# THIS MATCHES THE FILENAMES IN THE DOWNLOADED ZIP FOLDERS WE WANT TO PROCESS\n",
    "expenditure_qgdp_filename_list = [\n",
    "    \"ABJR - Household Final Consumption\",\n",
    "    \"BQKO - Imports of Goods\",\n",
    "    \"BQKQ - Exports of Goods\",\n",
    "    \"CAFU - Change in Inventories\",\n",
    "    \"HAYO - Non-Profit\",\n",
    "    \"IKBE - Export of Services\",\n",
    "    \"IKBF - Imports of Services\",\n",
    "    \"IKBK - Total Imports\",\n",
    "    \"NMRY - General Government\",\n",
    "    \"NPQT - Gross Fixed Capital\",\n",
    "]\n",
    "expenditure_finalpath = \"data/02_intermediate/expenditure/\"\n",
    "\n",
    "# MATCHES THE FILENAME OF THE DOWNLOADED XL FILE\n",
    "deflator_qgdp_filename_list = [\n",
    "    'ybgbgdpimplieddeflator.xlsx'\n",
    "]\n",
    "deflator_finalpath = \"data/02_intermediate/headline/\"\n",
    "\n",
    "# MATCHES THE FILENAME OF THE DOWNLOADED XL FILE\n",
    "income_qgdp_filename_list = [\n",
    "    \"CAEQ - Public corporations gross operating surplus.xlsx\",\n",
    "    \"CAER - Private non-financial corporations gross operating surplus.xlsx\",\n",
    "    \"CGBX - Other Income.xlsx\",\n",
    "    \"CMVL - Taxes less subsidies.xlsx\",\n",
    "    \"DTWM - Compensation of employees.xlsx\",\n",
    "    \"NHCZ - Financial corporations gross operating surplus.xlsx\",\n",
    "]\n",
    "income_finalpath = \"data/02_intermediate/income/\"\n",
    "\n",
    "# MATCHES THE FILENAME OF THE DOWNLOADED XL FILE\n",
    "investment_qgdp_filename_list = [\n",
    "    \"dataset5rftrt.xls\"\n",
    "]\n",
    "investment_finalpath = \"data/02_intermediate/headline/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipelines\n",
    "\n",
    "### Headline Quarterly GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and save to local folder\n",
    "headline_qgdp_raw = headline_qgdp_vintages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the latest release from the ONS website...\n",
      "Found the latest release URL: https://www.ons.gov.uk/file?uri=/economy/grossdomesticproductgdp/datasets/revisionstrianglesforukgdpabmi/quarter4octtodec2023firstestimate/abmi.zip\n",
      "Saving the raw data...\n",
      "Successfully saved the latest Headline Vintages data release from the ONS website.\n",
      "Loading the data...\n",
      "Cleaning the data...\n",
      "Transforming the data...\n",
      "Saving the data...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data\\02_intermediate\\headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\2. Models & code\\D&O\\revisions_toolkit\\revisions_toolkit.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m transformed_QGDP_df \u001b[39m=\u001b[39m transform_and_combine(clean_QGDP_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Save the output\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m save_data(transformed_QGDP_df, headline_qgdp_filename_list, headline_qgdp_finalpath)\n",
      "\u001b[1;32mn:\\CECD\\2. Models & code\\D&O\\revisions_toolkit\\revisions_toolkit.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving the data...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mfor\u001b[39;00m data, name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data_list, input_name_list):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39mwith\u001b[39;00m pd\u001b[39m.\u001b[39;49mExcelWriter(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00msave_path\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m_PROCESSED.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m    <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39mfor\u001b[39;00m sheet_name, df \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X10sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m             df\u001b[39m.\u001b[39mto_excel(writer, sheet_name\u001b[39m=\u001b[39msheet_name, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkbook\u001b[39;00m \u001b[39mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m engine_kwargs \u001b[39m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 61\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     62\u001b[0m     path,\n\u001b[0;32m     63\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m     64\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m     65\u001b[0m     if_sheet_exists\u001b[39m=\u001b[39;49mif_sheet_exists,\n\u001b[0;32m     66\u001b[0m     engine_kwargs\u001b[39m=\u001b[39;49mengine_kwargs,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[39m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode:  \u001b[39m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1246\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m IOHandles(\n\u001b[0;32m   1243\u001b[0m     cast(IO[\u001b[39mbytes\u001b[39m], path), compression\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m   1244\u001b[0m )\n\u001b[0;32m   1245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1247\u001b[0m         path, mode, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cur_sheet \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39mif\u001b[39;00m date_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    751\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data\\02_intermediate\\headline'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load up the data\n",
    "loaded_QGDP_df = load_data(headline_qgdp_raw, headline_qgdp_filename_list)\n",
    "\n",
    "# Clean the data\n",
    "clean_QGDP_df = clean_quarterly_data(loaded_QGDP_df)\n",
    "\n",
    "# Transform the data\n",
    "transformed_QGDP_df = transform_and_combine(clean_QGDP_df)\n",
    "\n",
    "# Save the output\n",
    "save_data(transformed_QGDP_df, headline_qgdp_filename_list, headline_qgdp_finalpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income QGDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and save to local folder\n",
    "income_qgdp_raw = income_qgdp_vintages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the latest release from the ONS website...\n",
      "Found the latest release URL: https://www.ons.gov.uk/file?uri=/economy/grossdomesticproductgdp/datasets/revisionstrianglesforcomponentsfortheincomeapproachtothemeasureofukgdp/quarter4octtodec2023firstestimate/income.zip\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='www.ons.gov.uk', port=443): Read timed out. (read timeout=5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:384\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    382\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in Python 3;\u001b[39;00m\n\u001b[0;32m    383\u001b[0m             \u001b[39m# otherwise it looks like a programming error was the cause.\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:380\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    382\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in Python 3;\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[39m# otherwise it looks like a programming error was the cause.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1164\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 439\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    440\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    441\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    442\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    443\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    444\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    445\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    449\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:637\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    635\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m'\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m'\u001b[39m, e)\n\u001b[1;32m--> 637\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    638\u001b[0m                             _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m])\n\u001b[0;32m    639\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\util\\retry.py:367\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[0;32m    368\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\packages\\six.py:686\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 686\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:597\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(conn, method, url,\n\u001b[0;32m    598\u001b[0m                                       timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    599\u001b[0m                                       body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    600\u001b[0m                                       chunked\u001b[39m=\u001b[39;49mchunked)\n\u001b[0;32m    602\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_timeout(err\u001b[39m=\u001b[39;49me, url\u001b[39m=\u001b[39;49murl, timeout_value\u001b[39m=\u001b[39;49mread_timeout)\n\u001b[0;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\urllib3\\connectionpool.py:306\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 306\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m, url, \u001b[39m\"\u001b[39m\u001b[39mRead timed out. (read timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m timeout_value)\n\u001b[0;32m    308\u001b[0m \u001b[39m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='www.ons.gov.uk', port=443): Read timed out. (read timeout=5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\2. Models & code\\D&O\\revisions_toolkit\\revisions_toolkit.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Extract the data and save to local folder\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m income_qgdp_raw \u001b[39m=\u001b[39m income_qgdp_vintages\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load up the data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m income_loaded_QGDP_df \u001b[39m=\u001b[39m load_data(income_qgdp_raw, income_qgdp_filename_list)\n",
      "\u001b[1;32mn:\\CECD\\2. Models & code\\D&O\\revisions_toolkit\\revisions_toolkit.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_release_date \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Send a GET request to the latest release URL and get the response\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m response \u001b[39m=\u001b[39m boerequests\u001b[39m.\u001b[39;49mget(latest_release_url, timeout\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Check if the request was successful\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X14sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39m# Load the data as a dictionary to hold each sheet within a df\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39m\u001b[39mget\u001b[39m\u001b[39m'\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\api.py:60\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    570\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    571\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    572\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    573\u001b[0m }\n\u001b[0;32m    574\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 575\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    577\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\sessions.py:688\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    687\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    690\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    691\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\revisions_toolkit\\lib\\site-packages\\boerequests\\adapters.py:529\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    528\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='www.ons.gov.uk', port=443): Read timed out. (read timeout=5)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load up the data\n",
    "income_loaded_QGDP_df = load_data(income_qgdp_raw, income_qgdp_filename_list)\n",
    "\n",
    "# Clean the data\n",
    "income_clean_QGDP_df = clean_quarterly_data(income_loaded_QGDP_df)\n",
    "\n",
    "# Transform the data\n",
    "income_transformed_QGDP_df = transform_and_combine(income_clean_QGDP_df)\n",
    "\n",
    "# Save the output\n",
    "save_data(income_transformed_QGDP_df, income_qgdp_filename_list, income_finalpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expenditure QGDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and save to local folder\n",
    "expenditure_qgdp_raw = expenditure_qgdp_vintages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the latest release from the ONS website...\n",
      "Found the latest release URL: https://www.ons.gov.uk/file?uri=/economy/grossdomesticproductgdp/datasets/revisionstrianglesforcomponentsfortheexpenditureapproachtothemeasureofukgdp/quarter4octtodec2023firstestimate/expenditure.zip\n",
      "Saving the raw data...\n",
      "Successfully saved the latest Expenditure Vintages data release from the ONS website.\n",
      "Loading the data...\n",
      "Cleaning the data...\n",
      "Transforming the data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'expenditure_qgdp_finalpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\2. Models & code\\D&O\\revisions_toolkit\\revisions_toolkit.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m expenditure_transformed_QGDP_df \u001b[39m=\u001b[39m transform_and_combine(expenditure_clean_QGDP_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Save the output\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/CECD/2.%20Models%20%26%20code/D%26O/revisions_toolkit/revisions_toolkit.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m save_data(expenditure_transformed_QGDP_df, expenditure_qgdp_filename_list, expenditure_qgdp_finalpath)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'expenditure_qgdp_finalpath' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load up the data\n",
    "expenditure_loaded_QGDP_df = load_data(expenditure_qgdp_raw, expenditure_qgdp_filename_list)\n",
    "\n",
    "# Clean the data\n",
    "expenditure_clean_QGDP_df = clean_quarterly_data(expenditure_loaded_QGDP_df)\n",
    "\n",
    "# Transform the data\n",
    "expenditure_transformed_QGDP_df = transform_and_combine(expenditure_clean_QGDP_df)\n",
    "\n",
    "# Save the output\n",
    "save_data(expenditure_transformed_QGDP_df, expenditure_qgdp_filename_list, expenditure_finalpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deflator QGDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and save to local folder\n",
    "deflator_qgdp_raw = deflator_qgdp_vintages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load up the data\n",
    "deflator_loaded_QGDP_df = load_data(deflator_qgdp_raw, deflator_qgdp_filename_list)\n",
    "\n",
    "# Clean the data\n",
    "deflator_clean_QGDP_df = clean_quarterly_data(deflator_loaded_QGDP_df)\n",
    "\n",
    "# Transform the data\n",
    "deflator_transformed_QGDP_df = transform_and_combine(deflator_clean_QGDP_df)\n",
    "\n",
    "# Save the output\n",
    "save_data(deflator_transformed_QGDP_df, deflator_qgdp_filename_list, deflator_qgdp_finalpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline Monthly GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and save to local folder\n",
    "headline_mgdp_raw = headline_mgdp_vintages.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the latest release from the ONS website...\n",
      "Found the latest release URL: https://www.ons.gov.uk/file?uri=/economy/grossdomesticproductgdp/datasets/revisionstrianglesformonthlygdp/current/mgdprevisiontriangle.zip\n",
      "Saving the raw data...\n",
      "Successfully saved the latest Monthly Vintages data release from the ONS website.\n",
      "Loading the data...\n",
      "Transforming the data...\n",
      "Saving the data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load up the data\n",
    "headline_loaded_MGDP_df = load_data(headline_mgdp_raw, headline_mgdp_filename_list)\n",
    "\n",
    "# Clean the data\n",
    "headline_clean_MGDP_df = clean_monthly_data(headline_loaded_MGDP_df)\n",
    "\n",
    "# Transform the data\n",
    "headline_transformed_MGDP_df = transform_and_combine(headline_clean_MGDP_df)\n",
    "\n",
    "# Save the output\n",
    "save_data(headline_transformed_MGDP_df, headline_mgdp_filename_list, headline_mgdp_finalpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "revisions_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
